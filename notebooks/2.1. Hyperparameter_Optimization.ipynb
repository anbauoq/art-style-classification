{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbddcba6",
   "metadata": {},
   "source": [
    "# Bayesian Hyperparameter Optimization\n",
    "For this image classification task, `EfficientNet-B0` model was selected. The final classifier is replaced with an 8-class linear head (producing 8 logits).\n",
    "\n",
    "Hyperparameters are tuned with Optuna to maximize macro-F1 on the validation set, a criterion suited to imbalanced labels, with the goal of robust generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb15634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import json\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "\n",
    "from src.data import get_datasets\n",
    "from src.utils import set_device, is_mbconv, EarlyStopper\n",
    "from src.train import train_step, val_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b7730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "DEVICE = set_device()\n",
    "\n",
    "datasets = get_datasets(root=\"../data/augmented_images\")\n",
    "output_dir = \"../outputs/param-optim\"\n",
    "full_train_ds = datasets[\"train\"]\n",
    "full_val_ds = datasets[\"val\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b48d0b9",
   "metadata": {},
   "source": [
    "I am going to use the Optuna framework for Bayesian hyperparameter optimization. Unlike a basic grid search, this method selects new hyperparameter combinations based on previous trial results, making the search much more efficient.\n",
    "\n",
    "Four main hyperparameters are tuned in this process:\n",
    "\n",
    "1. Learning Rate\n",
    "\n",
    "2. Batch Size\n",
    "\n",
    "3. Weight Decay\n",
    "\n",
    "4. Number of Unfrozen Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc12b9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters to be tuned\n",
    "TUNING_RANGES = {\n",
    "    \"LR\": {\"type\": \"float\", \"low\": 1e-4, \"high\": 1e-2, \"log\": True},\n",
    "    \"BATCH_SIZE\": {\"type\": \"categorical\", \"choices\": [16, 32, 64]},\n",
    "    \"WEIGHT_DECAY\": {\"type\": \"float\", \"low\": 1e-4, \"high\": 8e-3, \"log\": True},\n",
    "    \"UNFREEZE_K\": {\"type\": \"int\", \"low\": 2, \"high\": 8},\n",
    "}\n",
    "\n",
    "EPOCHS_PER_TRIAL = 7\n",
    "N_TRIALS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d5689",
   "metadata": {},
   "source": [
    "## 1. Per-trial training setup\n",
    "\n",
    "- **Loss:** Cross-entropy\n",
    "\n",
    "- **Optimizer:** AdamW\n",
    "\n",
    "- **LR schedule:** Cosine annealing\n",
    "\n",
    "- **Early stopping:** Monitors validation macro-F1 (mode = max) with patience and an overfitting check on (val_loss âˆ’ train_loss)\n",
    "\n",
    "- **Fine-tuning:** Progressive unfreezing of the last K MBConv blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"A single training run, called by Optuna for each trial.\"\"\"\n",
    "    \n",
    "    # 1. Suggest Hyperparameters Dynamically\n",
    "    params = {}\n",
    "    for name, p in TUNING_RANGES.items():\n",
    "        if p['type'] == 'categorical':\n",
    "            params[name] = trial.suggest_categorical(name, p['choices'])\n",
    "        elif p['type'] == 'int':\n",
    "            params[name] = trial.suggest_int(name, p['low'], p['high'])\n",
    "        elif p['type'] == 'float':\n",
    "            params[name] = trial.suggest_float(name, p['low'], p['high'], log=p.get('log', False))\n",
    "\n",
    "    # 2. Setup Model and Training Components\n",
    "    model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "    in_features = model.classifier[-1].in_features\n",
    "    model.classifier = nn.Linear(in_features, 8)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # --- Progressive Unfreezing Logic ---\n",
    "    for p in model.features.parameters(): p.requires_grad = False\n",
    "    mbconv_blocks = [m for m in model.features.modules() if is_mbconv(m)]\n",
    "    K = min(params['UNFREEZE_K'], len(mbconv_blocks))\n",
    "    for m in mbconv_blocks[-K:]:\n",
    "        for p in m.parameters(): p.requires_grad = True\n",
    "    for p in model.features[-1].parameters(): p.requires_grad = True\n",
    "    for p in model.classifier.parameters(): p.requires_grad = True\n",
    "        \n",
    "    # --- Dataloaders, Optimizer, etc. ---\n",
    "    pin = torch.cuda.is_available()\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        full_train_ds, \n",
    "        batch_size=params['BATCH_SIZE'], \n",
    "        shuffle=True, \n",
    "        pin_memory=pin\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        full_val_ds, \n",
    "        batch_size=params['BATCH_SIZE'],\n",
    "        pin_memory=pin\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=params['LR'], weight_decay=params['WEIGHT_DECAY'])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_PER_TRIAL)\n",
    "    \n",
    "    # 3. Training Loop\n",
    "    early_stopper = EarlyStopper(mode=\"max\", patience=3, max_overfit_gap=0.10)\n",
    "    best_f1 = 0.0\n",
    "    best_metrics_for_trial = {}\n",
    "\n",
    "    for epoch in range(1, EPOCHS_PER_TRIAL + 1):\n",
    "        train_metrics = train_step(model, train_loader, criterion, optimizer, DEVICE, disable_tqdm=True)\n",
    "        val_metrics = val_step(model, val_loader, criterion, DEVICE, disable_tqdm=True)\n",
    "        scheduler.step()\n",
    "        \n",
    "        val_f1 = val_metrics['f1_macro']\n",
    "        \n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            # You can keep this logic to store the best metrics for the trial\n",
    "            best_metrics_for_trial = {\n",
    "                \"val_f1_macro\": val_metrics['f1_macro'],\n",
    "                \"val_precision\": val_metrics['precision_macro'],\n",
    "                \"val_recall\": val_metrics['recall_macro'],\n",
    "                \"val_accuracy\": val_metrics['accuracy'],\n",
    "                \"val_loss\": val_metrics['loss'],\n",
    "                \"train_f1_macro\": train_metrics['f1_macro'],\n",
    "                \"train_accuracy\": train_metrics['accuracy'],\n",
    "                \"train_loss\": train_metrics['loss'],\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            \n",
    "        # Optuna pruning logic\n",
    "        trial.report(val_f1, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        # Early Stopping\n",
    "        _, should_stop, _ = early_stopper.step(\n",
    "            val_patience_metric=val_metrics['f1_macro'],\n",
    "            train_loss=train_metrics['loss'],\n",
    "            val_loss=val_metrics['loss']\n",
    "        )\n",
    "        if should_stop:\n",
    "            break\n",
    "    \n",
    "    trial.set_user_attr(\"best_metrics\", best_metrics_for_trial)\n",
    "    \n",
    "    return best_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010199bb",
   "metadata": {},
   "source": [
    "## 2. Run the Study\n",
    "To speed up the process, a MedianPruner is used to stop unpromising trials early.\n",
    "\n",
    "Additionally, through empirical observation, the model tends to overfit rapidly. To mitigate this, early stopping was applied based on the gap between the training and validation F1 macro scores, ensuring that training halts once overfitting becomes evident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c664ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence Optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "optuna.logging.disable_default_handler()\n",
    "\n",
    "# Create a study object and specify the direction is to maximize the F1 score\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "\n",
    "try:\n",
    "    # Start the optimization process\n",
    "    study.optimize(objective, n_trials=N_TRIALS, timeout=3600)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Study interrupted manually. Results up to this point will be shown.\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"HYPERPARAMETER TUNING COMPLETE\")\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a8491",
   "metadata": {},
   "source": [
    "## 3. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abdc006",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Best Trial ---\")\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(f\"Value (val_f1_macro): {best_trial.value:.4f}\")\n",
    "print(\"Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b8c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Full Metrics for Best Trial ---\")\n",
    "best_metrics = best_trial.user_attrs.get(\"best_metrics\", {})\n",
    "print(pd.Series(best_metrics).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea21377",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(output_dir, \"best_hyperparameters.json\")\n",
    "\n",
    "with open(file_path, 'w') as f:\n",
    "    json.dump(best_trial.params, f, indent=4)\n",
    "\n",
    "print(\"Best hyperparameters are saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9734fc",
   "metadata": {},
   "source": [
    "Those hyperparameters will be used for the training of the model for our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a0a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create a DataFrame with full results from all trials ---\n",
    "all_trials = []\n",
    "for trial in study.trials:\n",
    "    if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "        trial_metrics = trial.user_attrs.get(\"best_metrics\", {})\n",
    "        # Combine hyperparameters and the resulting metrics\n",
    "        row = {**trial.params, **trial_metrics}\n",
    "        all_trials.append(row)\n",
    "\n",
    "results_df = pd.DataFrame(all_trials)\n",
    "\n",
    "# Add the overfitting gap and sort\n",
    "if 'train_accuracy' in results_df.columns and 'val_accuracy' in results_df.columns:\n",
    "    results_df['overfit_gap'] = results_df['train_f1_macro'] - results_df['val_f1_macro']\n",
    "\n",
    "sorted_df = results_df.sort_values(by=\"val_f1_macro\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "pd.set_option('display.width', 120)\n",
    "display(sorted_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32148f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(output_dir, \"hyperparam-optim-results.csv\")\n",
    "\n",
    "sorted_df.to_csv(file_path)\n",
    "\n",
    "print(\"All results are saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows how the F1-score improved over trials\n",
    "fig = optuna.visualization.plot_optimization_history(study, target_name=\"val_f1_macro\")\n",
    "file_path = os.path.join(output_dir, \"optimization_history.png\")\n",
    "fig.write_image(file_path, width=1600, height=1000, scale=2)\n",
    "\n",
    "# Display it in the notebook\n",
    "Image(filename=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a342ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows which hyperparameters were most important\n",
    "fig = optuna.visualization.plot_param_importances(study, target_name=\"val_f1_macro\")\n",
    "file_path = os.path.join(output_dir, \"param-importances.png\")\n",
    "fig.write_image(file_path, width=1600, height=1000, scale=2)\n",
    "\n",
    "# Display it in the notebook\n",
    "Image(filename=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd383319",
   "metadata": {},
   "source": [
    "The hyperparameter importance analysis indicates that learning rate and batch size had the largest impact on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3087aa",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "Hyperparameters with the highest validation macro-F1 are saved and used to train the final model.\n",
    "Training converges in a few epochs (by epoch 2) with a modest trainâ€“validation gap (â‰ˆ0.07 F1; loss gap â‰ˆ0.17), indicating mild overfitting but solid validation performance.\n",
    "\n",
    "For final training, early stopping will monitor the loss gap between training and validation and stop if it grows too large (unlike the hyperparameter search, which used an F1-based criterion)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
